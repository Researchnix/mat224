\documentclass[letterpaper, 10pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{silence}
\WarningFilter{latex}{You have requested package}
\input{ltx/pkg/preamble}





\begin{document}

\lhead{MAT224 Linear Algebra II}
\chead{Linear Transformations, Image \& Kernel}
\rhead{Week 04}

\title{Linear Algebra II \\ \Large{MAT224}}
\author{Lennart Döppenschmitt}
% \maketitle
% \tableofcontents

\section*{Linear Transformations Part II}%
\label{sec:title}

\textbf{Textbook:} Section 2.2

\lb
\textbf{Announcements} 
\begin{itemize}
\item 
The last hour before the lecture next week will be a review session!
Please collect your questions and email me if you would like to discuss anything
particular. (include MAT224 in subject, thanks)

\item
All vector spaces from now on, unless stated otherwise,
will be assumed to be finite dimensional.
\end{itemize}








\lb
Remember that a basis encodes a vector $\vec v ∈ V$ as an $n$-tuple.
We can use the same idea to encode linear transformations



\lb
\textbf{Example 2.2.2}
\lb
Let $V = W = \R^2$ with the standard basis $ \cb{\vec e_1, \vec e_2}$.
Define $ \map{V}[T]{W}$ by
\[ T(\vec e_1) = \vec e_1 + \vec e_2 \]
\[ T(\vec e_2) = \vec 2 e_1 - 2 \vec e_2 \]
\vspace{50pt}








\newpage
\lb
\textbf{Algorithm}
\lb
Given a linear transformation $ \map{V}[T]{W}$ given as a `formula',
this is how to compute its matrix in two chosen bases
\[ α = \cb{\vec{α}_1, \ldots \vec{α}_m} \]
of $V$ and
\[ β = \cb{\vec{β}_1, \ldots \vec{β}_n} \]
of $W$
\lb
\begin{enumerate}
    \item For each basis element $\vec{α}_i$ in $V$, compute $T(\vec{α}_i)$.
    \item Find the coorindate vector $γ^β(T(\vec{α}_i)) = [ T(\vec{α}_i) ]_β$.
    \item Assemble these coordinate vectors as columns in a matrix
\end{enumerate}



\lb
\textbf{Discussion}
\lb
Apply the above algorithm to find the matrix representing the derivative
$ \frac{d}{dx}$ from $\pol{2}{\R}$ to itself.
Choose the basis on $\pol{2}{\R}$ consisting of monomials $α = \cb{1, x, x^2}$.




\newpage
\lb
\textbf{Definition 2.2.6}
\lb
Let $T$ be a linear transformation between finite dimensional vector spaces $V$ and $W$
with bases $α$ and $β$ respectively.
The \emph{matrix of the linear transformation} $T$ with respect to bases $α$ and $β$ is the
matrix $[T]_α^β$ satisfying
\[ [T]_α^β \cdot [\vec v]_α = [T(\vec v) ]_β \]





\newpage
\lb
\textbf{Discussion}
\begin{enumerate}
    \item
        What does he `size' of the matrix $[T]_α^β$ depend on?
    \item
        What is the matrix of the identity transformation $ \map{V}[\tx{id}_V]{V}$?
\end{enumerate}


\lb
\textbf{Exmaple}
\lb
We compute the matrix of the linear transformation $ \map{\pol{3}{\R}}[\tx{ev}_2]{\R}$



\newpage
\lb
\textbf{Discussion}
\lb
On the contrary, given a matrix $A ∈ \mat{n, m}{\R}$, does this give us a
linear transformation? What are the domain and codomain?


\vspace{300pt}
\lb
\textbf{Summary}
\begin{enumerate}
    \item
        The upshot of this section is that linear transformations are completely interchangable
        with matrices!
        The identification depends on the choice of bases for the domain and codomain.
    \item
        The operations
        \begin{itemize}
            \item matrix of a linear transformation $[T]_α^β$
            \item linear transformation of a matrix $T_A$
        \end{itemize}
        are inverse to each other.
\end{enumerate}




\newpage
\lb
We now rephrase the \emph{algorithm} from before in more mathematical terms.
(Remember, abstraction is a powerful tool!)
\lb
\textbf{Proposition} 
\lb
In the context of the above definition, the matrix of $T$ can be computed as
\[ [T]_α^β = γ^β \circ T \circ (γ^α)^{-1}\]
\begin{proof}
\end{proof}

\vspace{200pt}
\lb
\textbf{Discussion}
\lb
Without doing a lot of work, can you argue what the matrix
representing the composition $F \circ T$ is assuming you know $[F]$ and $[T]$?






\newpage

\lb
\textbf{Definition (2.3.1 \& 2.3.10)}
\lb
For a linear transformation $ \map{V}[T]{W}$, we define
\begin{enumerate}
    \item
        the \emph{preimage} $T^{-1}(S)$ of $S \subseteq W$ under $T$ as all $\vec v ∈ V$ that map into $S$.
    \item
        the \emph{kernel} $\ker(T)$ of $T$  as all $\vec v ∈ V$ that map to $\vec 0$ under $T$,
    \item 
        the \emph{image} $\tx{im}(T)$ of $T$ as all $\vec w ∈ W$ such that $\vec w = T(\vec v)$ for some $\vec v ∈ V$,
\end{enumerate}




\lb
\textbf{Example}
\begin{itemize}
    \item
    The kernel of $\map{\pol{n}{\R}}[\frac{d}{dx}]{\pol{n}{\R}}$ are all constant polynomials,
    while the image consists of polynomials of degree $n-1$.

    \item
    The kernel of the evaluation map $\map{\pol{n}{\R}}[\tx{ev}_2]{\pol{n}{\R}}$ are all
    polynomials that have a root at $x = 2$. What is the image?

    \item
    What is the image of the linear transformation defined in example 2.2.2 $\map{\R^2}[T]{\R^2}$?
    \[ T(\vec e_1) = \vec e_1 + \vec e_2 \]
    \[ T(\vec e_2) = \vec 2 e_1 - 2 \vec e_2 \]
\end{itemize}


\newpage
\lb
\textbf{Proposition 2.3.2 \& 2.3.11}
\lb
For every linear transformation $\map{V}[T]{W}$
\begin{enumerate}
    \item $\ker(T)$ is a subspace in $V$
    \item $\tx{im}(T)$ is a subspace in $W$.
\end{enumerate}
\begin{proof}
\end{proof}



\vspace{300pt}
\lb
\textbf{Proposition 2.3.7}
\lb
The subspace $\ker(T)$ is the solution space to the homogeneous system of $[T]_α^β$.
\begin{proof}
\end{proof}

\vspace{200pt}
\lb
\textbf{Example}
\lb
Example of computation to find $\ker(T)$




\newpage
\lb
\textbf{Observation}
\lb
The subspace $\tx{im}(T)$ is the space of all $ \vec b ∈ \R^n$ such that the system
$[T]_α^β \vec x = \vec b$ has a solution.


\vspace{200pt}
\lb
\textbf{Proposition 2.3.12}
\lb
If $ \cb{\vec v_1, \ldots, \vec v_k}$ spans $V$, then $ \cb{T(\vec v_1), \ldots, T(\vec v_k)}$
spans $\tx{im}(T)$.
\begin{proof}
\end{proof}




\vspace{200pt}
\lb
\textbf{Definition}
\lb
For a matrix $A = [ a_1, a_2, \ldots, a_m] ∈ \mat{n, m}{\R}$ we denote
the span of the columns of $A$ by
\[ \tx{col}(A) = \spn{a_1, \ldots, a_m} \]


\lb
\textbf{Proposition}
\lb
For every linear transformation $\map{V}[T]{W}$
\[ \tx{im}(T) = \tx{col}([T]_α^β) \]
\begin{proof}
\end{proof}



\newpage
\lb
\textbf{Example} 
\lb
Example computation to find $\tx{im}(T)$

\vspace{300pt}
\lb
Notice that the columns might not be independent,
in which case the columns are a spanning set of the image, but not a basis.

\lb
\textbf{Theorem}
\lb
Given a linear transformation $\map{V}[T]{W}$ with matrix $[T]_α^β$ for some bases $α$ and $β$.
Let $R = \tx{RREF}([T]_α^β)$ be the reduced row echolon form of $[T]_α^β$.
\lb
Then if the leading 1s in are $R$ lie in columns
$j_1, j_2,\ldots, j_r$, the columns $j_1, j_2,\ldots, j_r$ of $[T]_α^β$ are a basis for 
$ \tx{col}([T]_α^β)$
\begin{proof}
\end{proof}


\vspace{100pt}
\lb
\textbf{Discussion} 
\lb
Suppose a linear transformation $\map{V}[T]{W}$ is given in a some bases $α$ and $β$ by
\[ [T]_α^β = \begin{pmatrix} 1 & 2 & 0 & 1 \\ 1 & 2 & 1 & 0 \end{pmatrix} \]
Find a basis for $\tx{im}(T)$ and $\ker(T)$.










\newpage
\lb
\textbf{Theorem 2.3.17 (Rank-Nullity)}
\lb
For any linear transformation $\map{V}[T]{W}$
\[ \dim(V) = \dim(\ker(T)) + \dim( \tx{im}(T)) \]


\lb
\textbf{Remark} 
\begin{itemize}
    \item 
        $\dim( \tx{im}(T))$ is the same as the rank of $[T]_α^β$ and by abuse of
        notation also referred to as $\tx{rank}(T)$.
    \item
        Some books refer to $\dim(\ker(T))$ as the \emph{nullity} of $T$.
\end{itemize}

\begin{proof}
\end{proof}


\vspace{300pt}
\lb
\textbf{Theorem}
\lb
A linear transformation $T$ is injective if and only if $\ker(T) = \cb{\vec 0}$
\begin{proof}
\end{proof}



\newpage
\lb
\textbf{True or False} 
Let $\map{V}[T]{W}$ be a linear transformation
\begin{enumerate}
    \item[$\square$]
        If $T$ is an isomorphism, then $\dim(V) = \dim(W)$.
    \item[$\square$]
        If $\dim(V) > \dim(W)$, $T$ has to be injective.
    \item[$\square$]
\end{enumerate}



















\end{document}
