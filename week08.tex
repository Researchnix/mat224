\documentclass[letterpaper, 10pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{silence}
\WarningFilter{latex}{You have requested package}
\input{ltx/pkg/preamble}





\begin{document}

\lhead{MAT224 Linear Algebra II}
\chead{Determinants and Eigenvalues Pt I}
\rhead{Week 08}

\title{Linear Algebra II \\ \Large{MAT224}}
\author{Lennart Döppenschmitt}
% \maketitle
% \tableofcontents





\section*{The Determinant}%
\textbf{Textbook:} Sections 3.1, 3.2 \& 3.3



\lb
\textbf{Observation (3.1.6)}
\lb
When is a $2 \times 2$ matrix
$ A = \begin{pmatrix}
    a & b \\ c & d
\end{pmatrix}$
invertible?




\vspace{200pt}
\lb
\textbf{Definition}
\lb
We define the determinant funtion $\map{\mat{2}{\R}}[det]{\R}$ to be
$ det \begin{pmatrix}
    a & b \\ c & d
\end{pmatrix} = ad - b c $



\lb
\textbf{Remark}
\lb
\begin{enumerate}
    \item As we observed above, a $2 \times 2$ matrix is invertible if and only if $ det(A) \neq 0$.
    \item It is helpful to think about $A = [ ~ \vec x ~~ \vec y ~] $ to to consist of column
        vectors $\vec x, \vec y ∈ \R^2$.
\end{enumerate}



\vspace{100pt}
\lb
\textbf{Proposition (3.1.1)}
\lb
The are of the parallelogram spanned by vectors $\vec x$ and $\vec y$ in $\R^2$ is
equal to the determinant of the matrix $A =  [ ~ \vec x ~~ \vec y~ ] $.


\vspace{50pt}
\lb
From this it is clear that the area is nonzero
\pr
if and only if the vectors spanning the parallelogram
\pr
are linearly independent, that is, not parallel.






\newpage
\lb
\textbf{Observation}
\lb
Let $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ be a $2 \times 2$ matrix and denote
its columns as $\vec x = \ttpl{a}{c}$ and $\vec y = \ttpl{b}{d}$.

\lb
We can think of $A$ as the matrix of a linear transformation
\[ \R^2 \ra \R^2 \]
sending 
\begin{align*}
    \ttpl{1}{0} \mapsto \ttpl{a}{c} \\
    \ttpl{0}{1} \mapsto \ttpl{b}{d}
\end{align*}







\newpage
\lb
\textbf{Discussion}
\lb
Compute the determinant, draw the parallelogram and decide if the corresponding transformation
is invertible for the following matrices.
\begin{enumerate}
    \item
        \[ A = \begin{pmatrix} 1 & 2 \\ 3 & -4 \end{pmatrix} \]
    \vspace{200pt}
    \item
        \[ A = \begin{pmatrix} 4 & 2 \\ -2 & 1 \end{pmatrix} \]
\end{enumerate}





\newpage
\lb
\textbf{Remark}
\lb
\begin{enumerate}
    \item Let again $A = [ ~ \vec x ~~ \vec y ] $ be a $ 2 \times 2 $ matrix with columns
        $\vec x$ and $\vec y$.
        The determinant function as a funtion of the columns $ det(\vec x, \vec y)$
        has the following properties
        \begin{enumerate}
            \item linear in both arguments
            \vspace{50pt}
            \item alternating
            \vspace{50pt}
            \item normalized
            \vspace{50pt}
        \end{enumerate}
    \item
        The book calls this the area function $Area(\vec x , \vec y)$ of the vectors spanning
        the parallelogram.
\end{enumerate}



\vspace{200pt}
\lb
\textbf{Proposition (3.1.4)}
\lb
The determinant funtion is the unique funtion satisfying all properties above.
\begin{proof}
    In the book
\end{proof}






\vspace{100pt}
\lb
\textbf{Discussion}
\lb
Can you show that
\[ det( [~ \vec x ~~ \vec y ~ ] ) = 0 \]
if $ \cb{\vec x, \vec y}$ is linearly dependent by only using the properties above?






\newpage
\section*{Determinant of n x n matrices}%
\textbf{Textbook:} Sections 3.2

\lb
\textbf{Goal}
\lb
Generalize the det function to all $n \times n$ matrices as a function of $n$ vectors
$\vec x_1, \ldots, \vec x_n$ with an equivalent set of properties.
\begin{enumerate}
    \item multilinear
    \vspace{50pt}
    \item alternating
    \vspace{50pt}
    \item normalized
    \vspace{50pt}
\end{enumerate}

\lb
\textbf{Definition}
\lb
Let $A ∈ \mat{n}{\R}$ be a $n \times n$ matrix. Define the $ij$-minor $A_{ij}$ to be the
$ (n-1) \times (n-1)$ matrix obtained by deleting the $i$-th row and $j$th column.


\vspace{200pt}
\lb
\textbf{Discussion}
\lb
Compute for the matrix $A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{pmatrix}$ 
the minor $A_{23}$/






\newpage
\lb
\textbf{Proposition (3.2.5, 3.2.6)}
\lb
There exists a unique multilinear and alternating function $f$ on the columns of $3 \times 3$
matrices that is normalized such that $f(I_n) = 1$.


\lb
\textbf{Definition (3.2.7)}
\lb
We define the determinant of a $3 \times 3$ matrix to be this unique function.
\[ det(A) = a_{11} det(A_{11}) - a_{12} det(A_{12}) + a_{13} det(A_{13}) \]





\vspace{200pt}
\lb
\textbf{Discussion}
\lb
Compute the determinant of the matrix
\[ A = \begin{pmatrix} 1 & 0 & 1 \\ 2 & 3 & 1 \\ 0 & 3 & 2 \end{pmatrix} \]






\newpage
\textbf{Remarks}
\lb
\begin{enumerate}
    \item The formula through which we defined the determinant function
        is called the \emph{cofactor expansion}.
    \item It is also common to call
        \[ a_{ij} det(A_{ij}) \]
        the $ij$-\emph{cofactor} of a matrix $A$.
    \item
        There is nothing special about the first row, we can expand the $det(A)$ along
        any row or column.
        \lb
        \textbf{Example}
        \lb
        \[ A = \begin{pmatrix} 1 & 0 & 1 \\ 2 & 3 & 1 \\ 0 & 3 & 2 \end{pmatrix} \]
\end{enumerate}


\vspace{150pt}
\lb
\textbf{Definition} We define for an $n \times n$ matrix the determinant function
\[ det(A) = \Sum[j=1][n] (-1)^{i + j} a_{ij} det(A_{ij})\]
\lb
\textbf{Theorem 3.2.8}
\lb
\begin{enumerate}
    \item There is exactly one alternating multilinear function $\map{\mat{n}{\R}}[f]{\R}$
        such that $f(I_n) = 1$, which is the determinant function defined above.
    \item Any other alternating multilinear function $f$ on square $n \times n$ matrices
        satisfies
        \[ f(A) = det(A) \cdot f(I_n) \]
\end{enumerate}


\vspace{40pt}
\lb
\textbf{Discussion}
\lb
Write the first line of the cofactor expansion of the determinant for the matrix
\[ A = \begin{pmatrix} 
1 & -3 & 1 & 4 \\ 8 & -4 & 2 & -2 \\ 3 & -7 & 5 & 3 \\ 0 & 3 & -2 & 4 \end{pmatrix} \]




\newpage

\lb
\textbf{Exercise: } Read Example 3.2.9 in the book and practice computing determinants.
\lb

\vspace{100pt}
\lb
\textbf{Theorem (3.2.14)}
\lb
An $n \times n$ matrix $A$ is invertible if and only if $det(A) \neq 0$.
\begin{proof}
    In the book
\end{proof}

\lb
\textbf{Remark}
\lb
Notice that this agrees perfectly with the discussion above in the $2\times 2$ case.











\newpage
\section*{Further properties of determinants}%
\textbf{Textbook:} Sections 3.3


\lb
\textbf{Proposition (3.3.7)}
\lb
If $A$ and $B$ are $n \times n$ matrices, then
\begin{enumerate}
    \item $ det(AB) = det(A) det(B) $
    \item If $A$ is invertible, then $ det(\iv A) = \frac{1}{det(A)} $
\end{enumerate}
\begin{proof}
\end{proof}


This is very useful for transformations, because their matrix representative depends on a choice
of basis
\vspace{300pt}
\lb
\textbf{Corollary (3.3.8)}
\lb
For a linear transformation $\map{V}[T]{V}$ on a vector space of finite dimensional vector
space $V$
\[ det( [T]_α^α) = det([T_β^β) \]
for any two bases $α$ and $β$.
\begin{proof}
\end{proof}



\newpage
\lb
So we can define independently of the choses basis
\lb
\textbf{Definition (3.3.9)}
\lb
The determinant $det(T)$ of a linear transformation $\map{V}[T]{V}$ on a vector space of finite dimension
is the determinant of $[T]_α^α$ for any choice of basis $α$.







\vspace{200pt}
\lb
\textbf{Proposition (3.3.11)}
\lb
A linear transformation $\map{V}[T]{V}$ on a vector space of finite dimension is invertible
if and only if $det(T) \neq 0$.
\begin{proof}
\end{proof}




\vspace{200pt}
\lb
\textbf{Proposition (3.3.12)}
\lb
For two linear transformations $\map{V}[S]{V}$ and $\map{V}[S]{V}$ on a vector space of
finite dimension, then
\begin{enumerate}
    \item $ det(ST) = det(S) \cdot det(T)$
    \item if $T$ is an isomorphism, then $det(\iv T) = \frac{1}{det(T)}$
\end{enumerate}
\begin{proof}
\end{proof}






\newpage
\section*{Eigenvalues Pt I}%
\textbf{Textbook:} Section 4.1





\end{document}
