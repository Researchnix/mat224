\documentclass[letterpaper, 10pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{silence}
\WarningFilter{latex}{You have requested package}
\input{ltx/pkg/preamble}





\begin{document}

\lhead{MAT224 Linear Algebra II}
\chead{Bases, Dimension \& Linear Transformations}
\rhead{Week 03}

\title{Linear Algebra II \\ \Large{MAT224}}
\author{Lennart Döppenschmitt}
% \maketitle
% \tableofcontents

\section*{Bases and Dimension}%
\label{sec:title}

\textbf{Textbook:} Section 1.6
\lb
\textbf{Warning:} These notes contain probably more content than we can cover in one week.
Don't panic, I included some extra material for easy access and reference for you.


\lb
\textbf{Definition 1.6.1}
\lb
A family of vectors $\cal B$ in a vector space $V$ is called \emph{a basis of $V$} if
\begin{enumerate}
    \item $\cal B$ spans $V$
    \item $\cal B$ is linearly independent
\end{enumerate}



\lb
\textbf{Examples} 
\begin{enumerate}
    \item $ \cb{\vec e_1, \ldots, \vec e_n} $ is a basis of $\R^n$.
    \item We have seen last week that $ \cb{\ttpl{1}{1}, \ttpl{-1}{1}} $ is a basis of $\R^2$
    \item Which families of polynomials of the ones
        we have seen before are bases of $\pol{2}{\R}$?
        Can you write down a basis of $\pol{2}{\R}$ that does not contain any monomials?
\end{enumerate}
\lb
A vector space does not have just one unique basis as we can easily verify.



\lb
\textbf{Discussion}
\lb
Given the vector $\ttpl{5}{3} ∈ \R^2$, write down all linear combinations of
$ \cb{\ttpl{1}{1}, \ttpl{-1}{1}} $ equal to $\ttpl{5}{3} ∈ \R^2$.



\newpage

\lb
\textbf{Theorem 1.6.3}
\lb
A family of vectors $\cal B$ is a basis of $V$ if and only if every vector
$\vec v ∈ V$ can be written uniquely as a linear combination of vectors in
$\cal B$.

\begin{proof}
\end{proof}




\vspace{100pt}
\lb
\textbf{Theorem}  (Extend \& Reduce pt. II)
\lb
Let $V$ be a vector space with a finite spanning set.
\begin{enumerate}
    \item 
        Every linearly independent family $S$ in $V$ may be enlarged to
        a basis $\cal B$ containing $S$. (Theorem 1.6.6)
    \item
        Every family $S$ that spans $V$ may be reduced to a basis $\cal B$ contained in $S$.
\end{enumerate}

\begin{proof}
This follows from the \emph{Extend} and \emph{Reduce} theorems from last week
\end{proof}




\newpage
\lb
\textbf{Remark}
\begin{enumerate}
    \item 
        Why do we need a finite spanning set for $V$? Some vector spaces, such as
        $\pol{}{\R}$ can not be spanned by finitely many polynomials.
        Can you show why?
    \item
        Observe that a basis hits a sweet spot as it is not too
        large to contain redundant vectors, but also not too
        small to not span the vetcor space.
\end{enumerate}






\lb
\textbf{Corollary}
\lb
Every finitely spanned vector space has a basis.





\lb
\textbf{Discussion}
\lb
Consider the vectors $p(x) = 1 + x$ and $q(x) = 1 + x + x^2$ in $\pol{2}{\R}$.
Find a third vector $r(x)$ such that the family $\cb{p, q, r}$ is a basis for $\pol{2}{\R}$.







\newpage
\lb
Even though a basis is not unique to a vector space, we would like to extract
an invariant, a label, something that characterizes the vectors space.
This invariant is motivated by the Corollary following below.

\lb
\textbf{Theorem 1.6.10}
\lb
If $V$ is spanned by a family $S$ with $m$ elements, then no linearly
independent family $R$ in $V$ can have more than $m$ elements.
\begin{proof}
\end{proof}



\vspace{400pt}
\lb
\textbf{Corollary 1.6.11}
\lb
Any two bases $\cal B$ and $\cal B'$ of $V$ have the same number of elements



\newpage
\lb
\textbf{Definitions}
\begin{enumerate}
    \item If a vector space $V$ has a finite basis, we say that $V$ is
        \emph{finite dimensional}.
    \item For a finite dimensional vector space $V$, the \emph{dimension}
        of $V$
        \[ \dim(V) \]
        is the number of elements of a basis of $V$.
\end{enumerate}


\lb
\textbf{Discussion} 
\lb
What is the dimension of 
\begin{enumerate}
    \item[] $\dim( \R ) = $
    \item[] $\dim( \pol{n}{\R} ) = $
    \item[] $\dim( \mat{2}{\R} ) = $
    \item[] $\dim( \smat{2}{\R} ) = $
    \item[] $\dim( \amat{2}{\R} ) = $
\end{enumerate}
\lb
Remember that a matrix $A$ is symmetric if $\tp A = A$ and antisymmetric if $\tp A = -A$.



\vspace{100pt}
\lb
\textbf{Discussion}
\begin{itemize}
    \item
    Can you argue that if $U \subseteq V$ is a subspace then $\dim (U) \leq \dim(V)$?
    \item
    Is it on the contrary true that for every subspace $U \subseteq V$
    $\dim(U) = \dim(V)$ implies  $U = V$?
\end{itemize}




\newpage
\lb
The following result is very useful when we are looking for a basis
of a vector space that we already know the dimension of.
\lb
\textbf{Proposition} (Corollary of last discussion)
\lb
Let $V$ be a vector space of dimension $n$ with a family $S$ containing $n$ vectors,
then the following are equivalent.
\begin{enumerate}
    \item[(a)]
        $S$ is a basis of $V$
    \item[(b)]
        $S$ is linearly independent
    \item[(c)]
        $S$ spans $V$
\end{enumerate}
\begin{proof}
\end{proof}


\vspace{300pt}
\lb
\textbf{Discussion}
\lb
is the family of polynomials $ \cb{1 + 2x -x^2, 1 + x + x^2}$ is a basis for the subspace
$W = \cb{a + bx + cx^2 ∈ \pol{2}{\R} \vert -3a + 2b + c = 0}$?







\newpage
\lb
Since subspaces are vector spaces too, all constructions above apply also to subspaces.


\lb
\textbf{Discussion}
\lb
Let $U$ and $W$ be subspaces in $V$, can the sum $\dim(U) + \dim(W)$ be greater than $\dim(V)$?
\pr
Argue why it is true or find a counterexample.



\vspace{200pt}
\lb
\textbf{Example} (continued)
\lb
Consider again the subspaces of symmetric and anti symmetric $2 \times 2$ matrices,
$\smat{2}{\R}$ and $\amat{2}{\R}$ respectively, in the vector space of all $2 \times 2$
matrices $\mat{2}{\R}$.
Compute the intersection $\smat{2}{\R} \cap \amat{2}{\R}$.


\vspace{300pt}
\lb
This example can be abstractly formalized as
\lb
\textbf{Theorem 1.6.18}
\lb
Let $U$ and $W$ be finite dimensional subspaces of a vector space $V$, then
\[ \dim (U + W) = \dim (U) + \dim(W) - \dim (U \cap W) \]
\begin{proof}
    Read the proof in the textbook (voluntarily).
\end{proof}






\newpage
\lb
\textbf{Discussion}
\lb
Let $S = \cb{\vec v_1, \vec v_2, \vec v_3}$ be a spanning set of $V$.
\begin{enumerate}
    \item What are the possible dimensions of $V$?
    \item Suppose $ \cb{v_1, v_3}$ is linearly independent, what are the possible dimensions
        of $V$?
    \item Can you create a basis for $V$ from $S$?
\end{enumerate}



\vspace{150pt}
\lb
With the results from this section, we can immediately obtain the following.
\lb
\textbf{Corollary \& Discussion}
\lb
Let $V$ be an $n$-dimensional vector space and $S$ a linearly independent family with $l$
elements, then
\begin{enumerate}
    \item $ l\leq n$
    \item if $l = n$, $S$ is a basis of $V$.
\end{enumerate}


\vspace{150pt}
\lb
\textbf{Discussion}
\lb
Let $\vec v_1, \vec v_2, \vec v_3, \vec v_4$ be vectors in a vector space $V$ and define
the subspace $U = \spn{\vec v_1, \vec v_2, \vec v_3, \vec v_4 }$
\pr
Suppose
\[ \vec v_3 = \vec v_1 - \vec v_2 \]
\[ \vec v_4 = 2 \vec v_1 + 3 \vec v_2 - \vec v_3 \]
\begin{enumerate}
    \item What are the possible dimenions of $U$?
    \item Suppose $\dim(U) = 2$. Does this imply that $ \cb{\vec v_3, \vec v_4}$ is linealry
        independent?
\end{enumerate}











\newpage
\lb
\textbf{Textbook:} Section 2.1

\section*{Linear Transformatoins}%
\label{sec:Linear Transformatoins}

\lb
\textbf{Definition 2.1.1}
\lb
A function $\map{V}[T]{W}$ between vector spaces
$(V, +_V, \bullet_V)$ and $ (W, +_W, \bullet_W) $
is called \emph{linear} if
\begin{enumerate}
    \item $T(\vec u +_V \vec v) = T(\vec v) +_W T(\vec v)$ \quad for all $\vec u, \vec v ∈ V$
    \item $T(α \bullet_V \vec u) = α \bullet_W T(\vec v) $ \quad for all $\vec u ∈ V$ and $α ∈ \R$.
\end{enumerate}


\lb
\textbf{Remark}
\begin{itemize}
    \item We usually use the expression \emph{linear transformation}
        or short \emph{transformation} and not linear function.
    \item Notice that the operations $+$ and $\cdot$ are exactly what distinguishes vector spaces
        from sets. The requirements for a function to be linear guarantee that it mediates
        between the operations on the domain and the target.
\end{itemize}

\lb
\textbf{Examples}
\begin{enumerate}
    \item Consider the function $x \mapsto e^x$, are there vector space structures on $\R$ such
        that this defines a linear transformation?
\end{enumerate}


\lb
\textbf{Discussion}
\lb
Given a transformation $\map{V}[T]{W}$, is it true that 
\[ T \left( \Sum[i=1][k] α_i \vec v_i \right) =  \Sum[i=1][k] α_i T(\vec v_i) \qquad ?\]









\newpage
\lb
\textbf{Discussion}
\lb
Which of the following functions are linear transformations?
\begin{enumerate}
    \item
        $\map{\R^2}[T]{\R}$ defined by $T \left( \ttpl{x}{y} \right) = \tx{max}\cb{x, y}$
    \item
        The derivative $ \map{\pol{n}{\R}}[\frac{d}{dx}]{\pol{n}{\R}}$
    \[f(x) \mapsto \frac{df}{dx} (x)\]
    \item
        The transposition of a matrix $A \mapsto A^T$ as a function
        $\map{\mat{n}{\R}}[T]{\mat{n}{\R}}$
    \item
        The function $\map{\mat{n}{\R}}[RREF]{\mat{n}{\R}}$ that computes the reduced row
        echolon form of a matrix $A \mapsto \tx{RREF}(A)$.
    \item
        The evaluation function $\map{\pol{n}{\R}}[\tx{ev}_7]{\R}$ evaluating a polynomial at
        the value 7.
        \[ \tx{ev}_7(p) = p(7) \]
\end{enumerate}



\lb
\textbf{Proposition}
\lb
For any linear transformation $\map{V}[T]{W}$ we have
\begin{enumerate}
    \item $T(\vec 0_V) = \vec 0_W$
    \item $T(-\vec v) = - T(\vec v)$
\end{enumerate}

\lb
Is the converse also true?



\newpage
\lb
\textbf{Definition}
\begin{enumerate}
    \item
        A transformation that is bijective is called an \emph{isomorphism}. If there exists
        and isomorphism between two vector spaces $V$ and $W$, they are called \emph{isomorphic}.
    \item
        A transformation $\map{V}[T]{V}$ for which domain and codomain agree is called
        an \emph{endomorphism}.
    \item
        Given two vector space $V$ and $W$, the set of all linear
        transformations from $V$ to $W$ is denoted
        \[ \mathcal{L} (V, W) \]
        or just $\mathcal{L}(V)$ if $V = W$.
\end{enumerate}


\lb
\textbf{Exercise}
\lb
Show that for any vector spaces $V$ and $W$ the set $\mathcal{L}(V,W)$ is a
vector space.
\pr
\emph{Hint:} Compare this to the vector space $\mathcal{F}(\R,\R)$ of
functions from $\R$ to $\R$.


















\newpage



\lb
Recall that a basis in a vector space enbales us to write every vector uniquely as
a linear combination. Since a linear combination is determined by its coefficients,
all we need to remember for a fixed basis is the n-tuple of coefficients.
To formalize this, we define...
\lb
\textbf{Definition}
\lb
Let $V$ be a vector space with a basis $α = \cb{\vec b_1, \ldots, \vec b_n}$ and for
$\vec v ∈ V$
\[ \vec v = v_1 \cdot \vec b_1 + \cdots + v_n \cdot \vec b_n \]
\underline{the} unique representation in this basis.
We call $v_1, \ldots, v_n$ the
\emph{coordinates} of $\vec v$ in the basis $α$ and
$[\vec{v}]_α = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}$
the \emph{coordinate vector} of $\vec v$ in the basis $\cal B$.

\lb
\textbf{Discussion}
\lb
Consider the basis $α = \cb{1, 1+x, 1 + x + x^2}$ of $\pol{2}{\R}$.
\begin{enumerate}
    \item
        Determine $[1]_α$, $[x]_α$,
        $ [x^2]_α$ and $ [1 + x]_α$.
    \item
        Determine $[a + bx + cx^2]_α$
        for arbitrary scalars $a, b, c ∈ \R$ and show that
        \[
        [a + bx + cx^2]_α
        = a[1]_α
        + b[x]_α
        + c[x^2]_α
        \]
\end{enumerate}








\newpage
\lb
\textbf{Theorem}
\lb
Given an $n$-dimensional vector space $V$ with a basis $ \cal B$. The assignment of
its coordinate vector $[\vec v]_α$ to every vector $\vec v ∈ V$ is a linear
transformation
\[ \map{V}[γ^α]{\R^n} \]
Even more is true, $γ^α$ is an isomorphism from $V$ to $\R^n$.
\begin{proof}
\end{proof}

\vspace{300pt}
\lb
\textbf{Intuition}
\lb
Isomorphic vector spaces $V$ and $W$ can be regared as practically the same vector space,
because we can use the isomorphism to go back and forth between elements from $V$ to $W$
bijectively. Moreover, an isomorphism identifies the operations on $V$ and $W$.

\lb
The above theorem says that every vector space of dimension $n$ is isomoprhic to $\R^n$ and
that the choice of a basis (there are many) tells us how to uniquely identify vectors
$\vec v ∈ V$ with $n$-tuples $(v_1, \ldots, v_n) ∈ \R^n$.


\lb
That is to say $\R^n$ is the prototypical $n$-dimensional vector space.














\newpage
\lb
\textbf{Discussion}
\lb
Suppose $ \map{\pol{2}{\R}}[T]{\R}$ is given by 
\[ T( 1 + x^2) = 5 \]
\[ T( x - x^2) = 3 \]
\[ T( 1 ) = 1 \]
What is $T(x)$ ?


\vspace{300pt}
\lb
\textbf{Proposition}
\lb
A transformation $\map{V}[T]{W}$ is uniquely determined by its values on elements
of a basis on $V$.
\begin{proof}
\end{proof}





\end{document}
