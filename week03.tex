\documentclass[letterpaper, 10pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{silence}
\WarningFilter{latex}{You have requested package}
\input{ltx/pkg/preamble}





\begin{document}

\lhead{MAT224 Linear Algebra II}
\chead{Bases, Dimension \& Linear Transformations}
\rhead{Week 03}

\title{Linear Algebra II \\ \Large{MAT224}}
\author{Lennart Döppenschmitt}
% \maketitle
% \tableofcontents

\section*{Bases and Dimension}%
\label{sec:title}

\textbf{Textbook:} Section 1.6


\lb
\textbf{Definition 1.6.1}
\lb
A family of vectors $\cal B$ in a vector space $V$ is called \emph{a basis of $V$} if
\begin{enumerate}
    \item $\cal B$ spans $V$
    \item $\cal B$ is linearly independent
\end{enumerate}



\lb
\textbf{Examples} 
\begin{enumerate}
    \item $ \cb{\vec e_1, \ldots, \vec e_n} $ is a basis of $\R^n$.
    \item We have seen last week that $ \cb{\ttpl{1}{1}, \ttpl{-1}{1}} $ is a basis of $\R^2$
    \item Which of families of polynomials we have seen before is a basis of $\pol{2}{\R}$?
        Can you write down a basis of $\pol{2}{\R}$ that does not contain any monomials?
\end{enumerate}



\lb
\textbf{Theorem 1.6.3}
\lb
A family of vectors $\cal B$ is a basis of $V$ if and only if every vector
$\vec v ∈ V$ can be written uniquely as a linear combination of vectors in
$\cal B$.

\begin{proof}
\end{proof}



\lb
\textbf{Remark} 
\begin{enumerate}
    \item A vector space does not have just one unique
        basis as we can easily verify.
\end{enumerate}


\lb
\textbf{Theorem 1.6.6} 
\lb
Let $V$ be a vector space with a finite spanning set.
For every linearly independent family $S$ in $V$, there
is a basis $\cal B$ containing $S$.

\lb
Why do we need a finite spanning set for $V$? Some vector spaces, such as
$\pol{}{\R}$ can not be spanned by finitely many polynomials.
Can you show why?


\lb
Observe that a basis hits a sweet spot of a family that is not too
large as that it would contain redundant vectors, but also not too
small of a family that it couldn't span the vetcor space.
\pr
The \emph{Extend} and \emph{Reduce} theorems from last week
give us the following:
\lb
A family of vectors that is linearly independent, but not a spanning set can
be enlagred to a basis, a family that spans $V$, but is linearly
dependent, can be cut down to form a basis.


\newpage
\lb
Even though a basis is not unique to a vector space, we would like to extract
an invariant, a label, something that characterizes the vectors space.
This invariant is motivated by the Corollary following below.

\lb
\textbf{Theorem 1.6.10} 
\lb
If $V$ is spanned by a family $S$ with $m$ elements, then no linearly
independent family $R$ in $V$ can have more than $m$ elements.
\begin{proof}
    
\end{proof}


\lb
\textbf{Corollary 1.6.11}
\lb
Any two bases $\cal B$ and $\cal B'$ of $V$ have the same number of elements



\lb
\textbf{Definitions} 
\begin{enumerate}
    \item If a vector space $V$ has a finite basis, we say that $V$ is
        \emph{finite dimensional}.
    \item For a finite dimensional vector space $V$, the \emph{dimension}
        of $V$
        \[ \dim(V) \]
        is the number of elements of a basis of $V$.
\end{enumerate}



\lb
\textbf{Discussion} 
\lb
What is the dimension of 
\begin{enumerate}
    \item[] $\dim( \R ) = $
    \item[] $\dim( \pol{n}{\R} ) = $
    \item[] $\dim( \mat{2}{\R} ) = $
    \item[] $\dim( \smat{2}{\R} ) = $
    \item[] $\dim( \amat{2}{\R} ) = $
\end{enumerate}
\lb
Remember that a matrix $A$ is symmetric if $\tp A = A$ and antisymmetric if $\tp A = -A$.



\lb
\textbf{Discussion}
\begin{itemize}
    \item
    Can you argue that if $U \subseteq V$ is a subspace then $\dim (U) \leq \dim(V)$?
    \item
    Is it on the contrary true that for every subspace $U \subseteq V$
    $\dim(U) = \dim(V)$ implies  $U = V$?
\end{itemize}




\newpage
\lb
The following result is very useful when we are looking for a basis
of space that we already know the dimension of.
\lb
\textbf{Proposition}
\lb
Let $V$ be a vector space of dimension $n$ with a family $S$ containing $n$ vectors,
then the following are equivalent.
\begin{enumerate}
    \item[(a)]
        $S$ is a basis of $V$
    \item[(b)]
        $S$ is linearly independent
    \item[(c)]
        $S$ spans $V$
\end{enumerate}


\newpage

\lb
Since subspaces are vector spaces too, all constructions above apply, to subspaces.


\lb
\textbf{Discussion}
\lb
Let $U$ and $W$ be subspaces in $V$, can the sum $\dim(U) + \dim(W)$ be greater than $\dim(V)$?
\pr
Argue why it might be true or find a counter example.


\lb
\textbf{Example} (continued)
\lb
Consider again the subspaces of symmetric and anti symmetric $2 \times 2$ matrices,
$\smat{2}{\R}$ and $\amat{2}{\R}$ respectively, in the vector space of all $2 \times 2$
matrices $\mat{2}{\R}$.
Compute the intersection $\smat{2}{\R} \cap \amat{2}{\R}$.


\lb
As a final result, we will state without proof
\lb
\textbf{Theorem 1.6.18}
\lb
Let $U$ and $W$ be finite dimensional subspaces of a vector space $V$, then
\[ \dim (U + W) = \dim (U) + \dim(W) - \dim (U \cap W) \]
\begin{proof}
    Read the proof in the textbook (voluntarily).
\end{proof}


\lb
\textbf{Corollary}
\lb
Every finite dimensional vector space has basis.
















\newpage
\lb
\textbf{Textbook:} Section 2.1

\section*{Linear Transformatoins}%
\label{sec:Linear Transformatoins}

\lb
\textbf{Definition 2.1.1}
\lb
A function $\map{V}[T]{W}$ between vector spaces
$(V, +_V, \bullet_V)$ and $ (W, +_W, \bullet_W) $
is called \emph{linear} if
\begin{enumerate}
    \item $T(\vec u +_V \vec v) = T(\vec v) +_W T(\vec v)$ \quad for all $\vec u, \vec v ∈ V$
    \item $T(α \bullet_V \vec u) = α \bullet_W T(\vec v) $ \quad for all $\vec u ∈ V$ and $α ∈ \R$.
\end{enumerate}


\lb
\textbf{Remark}
\begin{itemize}
    \item We usually use the expression \emph{linear transformation}
        or short \emph{transformation} and not linear function.
    \item Notice that the operations $+$ and $\cdot$ are exactly what distinguishes vector spaces
        from sets. The requirements for a function to be linear guarantee that it mediates
        between the operations on the domain and the target.
\end{itemize}

\lb
\textbf{Examples}
\begin{enumerate}
    \item ...tbd
\end{enumerate}


\lb
\textbf{Discussion}
\lb
Given a transformation $\map{V}[T]{W}$, is it true that 
\[ T \left( \Sum[i=1][k] α_i \vec v_i \right) =  \Sum[i=1][k] α_i T(\vec v_i) \qquad ?\]


\lb
\textbf{Discussion}
\lb
Which of the following functions are linear transformations?
\begin{enumerate}
    \item The derivative of a polynomial $\map{\pol{n}{\R}}[]{\pol{n}{\R}}$
            $p(x) \mapsto \frac{d}{dx} p(x)$
\end{enumerate}



\lb
\textbf{Proposition}
\lb
For any linear transformation $\map{V}[T]{W}$, $T(\vec 0_V) = \vec 0_W$.


\lb
\textbf{Proposition}
\lb
A transformation $\map{V}[T]{W}$ is uniquely specified by its values on elements of a basis on $V$.


\newpage



\lb
Recall that a basis in a vector space enbales us to write every vector uniquely as
a linear combination. Since a linear combination is determined by its coefficients,
all we need to remember for a fixed basis is the n-tuple of coefficients.
To formalize this, we define...
\lb
\textbf{Definition}
\lb
Let $V$ be a vector space with a basis $\mathcal{B} = \cb{\vec b_1, \ldots, \vec b_n}$ and for
$\vec v ∈ V$
\[ \vec v = v_1 \cdot \vec b_1 + \cdots + v_n \cdot \vec b_n \]
\underline{the} unique representation in this basis.
We call $[\vec{v}]_{\cal B} = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}$
the \emph{coordinate vector} of $\vec v$ in the basis $\cal B$.


\lb
\textbf{Proposition}
\lb
Given an $n$-dimensional vector space $V$ with a basis $ \cal B$. The assignment of
its coordinate vector $[\vec v]_{\mathcal{B}}$ to every vector $\vec v ∈ V$ is a linear
transformation
\[ \map{V}[γ^{\mathcal{B}}]{\R^n} \]








\end{document}
